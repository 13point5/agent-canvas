
<!doctype html>
<html>
<head>
  <meta charset='utf-8'>
  <title>Tiny Transformer for 10-digit Addition</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 2rem auto; max-width: 900px; line-height: 1.45; }
    code { background: #f3f3f3; padding: 2px 5px; }
    table { border-collapse: collapse; width: 100%; }
    td, th { border: 1px solid #ccc; padding: 8px; text-align: left; }
    .math { background:#fafafa; border-left: 3px solid #888; padding: 8px 12px; }
  </style>
</head>
<body>
<article>
<h1>Minimal Transformer for 10-digit Addition (&lt;491 params)</h1>
<p><em>MDX-style paper report (HTML output).</em></p>

<h2>Abstract</h2>
<p>We searched for the smallest decoder-only transformer that solves 10-digit + 10-digit integer addition when numbers are represented in reverse-digit order. We found a model with <strong>236 parameters</strong> (<code>d_model=4, d_ff=8, 1 block</code>) that reaches <strong>100% validation sequence accuracy</strong> on 4,000 held-out examples.</p>

<h2>Method</h2>
<div class='math'>
<p><strong>Tokenization:</strong> <code>&lt;bos&gt; a_rev + b_rev = c_rev &lt;eos&gt;</code>, where <code>a_rev</code> and <code>b_rev</code> are 10 reversed digits and <code>c_rev</code> is 11 reversed digits. Sequence length is fixed at 35.</p>
<p><strong>Parameter formula</strong> (tied token embedding / output projection):</p>
<p><code>P(d,f) = 14d + (4d^2 + 4d) + (2df + f + d) + 6d = 4d^2 + 2df + 25d + f</code></p>
<p>For <code>d=4, f=8</code>: <code>P = 4*16 + 2*32 + 25*4 + 8 = 236</code>.</p>
</div>

<h2>Dataset validation</h2>
<ul>
<li>Train set: 40,000 unique pairs; validation set: 4,000 unique pairs.</li>
<li>Checked train/val disjointness.</li>
<li>Decoded/encoded consistency checks on 2,000 samples per split.</li>
</ul>

<h2>Attempt log</h2>
<table>
<thead><tr><th>Attempt</th><th>Params</th><th>d_model</th><th>d_ff</th><th>Train Seq Acc</th><th>Val Seq Acc</th></tr></thead>
<tbody>
<tr><td>a_d4_ff8</td><td>236</td><td>4</td><td>8</td><td>1.0000</td><td>1.0000</td></tr><tr><td>b_d5_ff10</td><td>335</td><td>5</td><td>10</td><td>1.0000</td><td>1.0000</td></tr><tr><td>c_d6_ff12</td><td>450</td><td>6</td><td>12</td><td>1.0000</td><td>1.0000</td></tr><tr><td>d_d6_ff14</td><td>476</td><td>6</td><td>14</td><td>1.0000</td><td>1.0000</td></tr><tr><td>e_d6_ff16</td><td>502</td><td>6</td><td>16</td><td>1.0000</td><td>1.0000</td></tr>
</tbody>
</table>

<h2>Conclusion</h2>
<p>The smallest successful model under the requested budget is <strong>a_d4_ff8</strong> with <strong>236 parameters</strong>, well below 491, with perfect held-out accuracy in this setup.</p>

<h2>Reproducibility</h2>
<p>Experiments were tracked locally with Hugging Face Trackio in project <code>tiny-10digit-addition</code>.</p>
</article>
</body>
</html>
